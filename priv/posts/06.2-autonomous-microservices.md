---
title: Autonomous Microservices - Outbox Pattern
date: 2020-02-15 22:40
tags: architecture,article,en-US
---

# Autonomous Microservices - Outbox Pattern

In a microservice architecture the most of its complexity is in the consistency of the data, the question is, what's the best way
to communicate microservices so that one gets data from the other? How to expose in a single endpoint the relationship between resources?
How to deal with 2PC(Two Phases Commit)/Saga? In this article I will try to answer these questions.

Well, before we get into the nuances of the solution to the questions above, let's understand what's the real problem.

## The context

When using an architecture based on microservices, usually the first fact that comes to mind is to separate services by domain,
that is, taking e-commerce as an example, we could have a microservice for **Users** and another for **Orders**. So, let's imagine this scenario.

![Context](/images/microsservicos-autonomos/context.png)

## The Problem

Imagine an situation where is needed return the relationship between **users** and **orders**. Normally, there is two ways:

- Make two requests(one to capture users and the other one to capture each person's orders).
- Create a BFF (Backend For Front-end) that makes the above step.

![Options](/images/microsservicos-autonomos/options.png)

However, what if our business rule is that for each **new user** we need to create a **new order**?

> _"Well, but it's easy! Just propagate an event in the message broker when creating a user"_

Good! But, if the process of creating an _order_ fails, the new user should be removed as well...

> _"Just use the [Saga Pattern](https://microservices.io/patterns/data/saga.html)"_

![Saga](/images/microsservicos-autonomos/saga.png)

Well, if you needs to merge other services under this rule, your architecture will be exponentially complex.
There is **Pattern Outbox** for that; over the course of this article, we will understand why dealing with eventual consistency is good and allows you to scale in an architecture that have tons of business rule.

Normally, microservices have their own data store, in our context, _Users_ have their database and _Orders_ too.
However, in the above example we deal with dual write, that is, the simple creation of users can lead to possibles inconsistencies.
The reason is that we aren't able to have a shared transaction that spans the service's database and the Message Broker as well, as the latter cannot be analyzed in distributed transactions.
Therefore, in unfortunate and possible circumstances
Portanto, em circunstâncias infelizes e possíveis, may happen that the new user is created in the database, but the message has not sent in the message broker (for example, due some network problem).
The inverse is true also, we could sent the message to the Message Broker, but we are unable to store the user in the database.

Both situations are undesirable; this can cause no order to be created for a user apparently successfully created.
Or the order is created, but there is no user correlated for that.

Remember: One of the main concepts of microservices is the resilience, so if your service depends of others services to work properly, there is no microservices architecture, but a **microlithic architecture** as I affectionately call it.

## Teorema CAP

Este Teorema afirma que é impossível que o armazenamento de dados distribuídos forneça simultaneamente mais de duas das três garantias seguintes:

- Consistência
- Disponibilidade
- Partição tolerante a falhas

### Consistência

Uma garantia que cada nó em um cluster distribuído **retorne a escrita mais rescente**. Consistência refere-se a cada cliente ter a mesma visualização dos dados.
Existem vários tipos de modelos de consistência. A consistência no CAP (usada para provar o teorema) refere-se à linearizabilidade ou consistência sequencial, uma forma muito forte de consistência.

### Disponibilidade

Cada nó que não falha retorna uma resposta para **todas** as solicitações de leitura e escrita em um período de tempo razoável.
Para se considerar disponível, todos os nós (em ambos os lados de uma partição de rede) **devem** responder em um curto espaço de tempo.

### Partição tolerante a falhas

Um sistema que é _"partição tolerante a falhas"_ pode sustentar quaisquer quantidade de falhas na rede que não resulta em uma falha na rede completa.
Os dados são replicados entre os nós/rede para manter o sistema ativo por interrupções intermitentes.
Ao lidar com sistemas distribuídos modernos, **a Tolerância de Partição não é uma opção, é uma necessidade**. Portanto, temos que _negociar_ entre consistência e disponibilidade.

## Escolhendo

**Consistência + Disponibilidade:** Comumente utilizado em sistemas que necessitam de forte consistência e alta disponibilidade, chamados também de abordagem
Otimista, pois partem do pressuposto que as escritas NUNCA falham.

**Consistência + Partição Tolerante:** Sistemas que optam por essa estratégia, precisam abrir mão da disponibilidade. Existe uma palavra, bastante importante nesse contexto, chamada: Conscenso.
Em sistemas do tipo CP, caso exista um particionamento a escrita -- como mencionado anteriormente, pode sim ser rejeitada/negada.

**Disponibilidade + Partição Tolerante:** Utilizado quando a necessidade maior é disponibilidade e escalabilidade, portanto, abrindo mão da consistência,
lidando com a famosa **Consistência Eventual**, ou seja, deixando seus dados serem consolidados entre nós indefinidamente.
-- _Será sobre essa escolha que vamos discorrer_.

## Youtube Use Case

O Youtube precisa mentir para seus usuários, e explico o porque. Quando lidamos com a massa de dados que o Youtube possuí, falar em escalabilidade
é uma das tarefas mais difíceis, portanto, seguindo o Teorema CAP, devemos optar por **Disponibilidade e Partição Tolerante** sem titubiar. Pois,
imagina centralizar os dados em um local só ou fazer com que eventualmente apareça um erro na tela do usuário? Portanto, vamos a algumas opções óbvias:

- Compartilhar a mesma base de dados para quaisquer aplicação -- E está será o eterno problema.
- Deixar um servidor processando as requisições para recuperar os dados. -- E este servidor será o problema.
- Manter mais de um servidor de dados, fazendo com que eventualmente os dados sejam consolidados entre todos os servidores. -- Winner

É por isso que se duas pessoas entrarem em um vídeo em alta ao mesmo tempo irão ver visualizações com números distintos (claro, se cairem em nós distintos).

> _"Mas como lidar com a Consistência Eventual, e se precisar do dado atualizado?"_

Bom, como dito anteriormente, nada é uma bala de prata, tudo depende do problema arquitetural a se resolver.
O Domínio Users sempre contará com o dado atual **como se fosse o mais recente**, afinal ele não conhece outros domínios.
Portanto, se por alguma razão seja necessário obter os dados em **TEMPO REAL** pode ser que esta abordagem não sirva para
a resolução do problema -- Mas cá entre nós, na maioria das vezes não é necessário nem escalável tal necessidade.

## Consistência Eventual

> _"Eventual consistency is a consistency model used in distributed computing to achieve high availability that informally guarantees that, if no new updates are made to a given data item,
eventually all accesses to that item will return the last updated value. Eventual consistency, also called optimistic replication, is widely deployed in distributed systems,
and has origins in early mobile computing projects. A system that has achieved eventual consistency is often said to have converged, or achieved replica convergence.
Eventual consistency is a weak guarantee – most stronger models, like linearizability are trivially eventually consistent, but a system that is merely eventually consistent does not usually fulfill these stronger constraints."_ -- [Wikipedia](https://en.wikipedia.org/wiki/Eventual_consistency)

A ideia desse padrão é que quaisquer informações que o serviço X necessite contenha em seu próprio domínio.
Em nosso contexto, vamos imaginar que precisamos do **nome completo** do usuário ao criar uma **Order**, portanto, para o **domínio Order**
usuário é apenas um conjunto de **name** onde entra o chamado [**Bounded Context**](https://martinfowler.com/bliki/BoundedContext.html).

![Example Table](/images/microsservicos-autonomos/table-replication.png)

Perceba que os dados estarão replicados em ambos serviços (_email, full_name_), e isso é bom! Dessa forma garantimos a consistência local e temos
a clamada transação local.

## Consolidandos os Dados

Idealizamos, e agora?

Agora, apresentarei o famigerado Debezium e seu poder de [CDC (Change Data Capture)](https://en.wikipedia.org/wiki/Change_data_capture)

A ideia do Debezium é simples, ele atua no topo do Apache Kafka, ou seja imagine uma trigger de inserção na base de dados,
escutando quaisquer mudanças na tabela (dependendo da configuração) e as replicando no broker. Ou seja, por definição expomos os dados que quisermos
 -- Amplamente utilizado uma table **outbox** com o schema abaixo [ref](https://thoughts-on-java.org/outbox-pattern-hibernate/)

![outbox table](/images/microsservicos-autonomos/outbox-table.jpeg)
![debezium example](/images/microsservicos-autonomos/debezium.png)

Não me prolongarei sobre como o Debezium faz, pra isso existe sua excelente [documentação](https://debezium.io/documentation/).

Para mostrar na prática, fiz um exemplo bem simples [aqui](https://github.com/RafaelGSS/microservice-debezium-outbox) basta seguir as instruções do README, e voilà!

Qualquer dúvida ~~guarde pra você~~ fale comigo em qualquer [rede social](https://rafaelgss.github.io/portfolio/).

